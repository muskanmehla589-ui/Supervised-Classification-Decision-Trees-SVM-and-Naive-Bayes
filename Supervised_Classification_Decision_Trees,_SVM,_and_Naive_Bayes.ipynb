{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Information Gain measures how much uncertainty (impurity) in the data is reduced after splitting on a particular feature.\n",
        "\n",
        "Information Gain is a concept used in Decision Tree algorithms to decide which feature should be chosen to split the data at each step.\n",
        "\n",
        "**How It Works in Decision Trees**\n",
        "\n",
        "- A decision tree starts with a mixed dataset where different classes are present.\n",
        "\n",
        "- For each feature, the algorithm checks how well it separates the data into clear groups.\n",
        "\n",
        "- If a feature divides the data into mostly single-class groups, it has high Information Gain.\n",
        "\n",
        "- The feature with the highest Information Gain is chosen as the decision node.\n",
        "\n",
        "- This process continues until the data becomes pure or stopping conditions are met.\n",
        "\n",
        "If we are predicting whether people will play a game, a feature like weather may clearly separate “yes” and “no” outcomes, while a feature like day number may not.\n",
        "So, weather would have higher Information Gain and be chosen first.\n",
        "\n",
        "**Information Gain measures how much a feature reduces uncertainty in the data and is used in decision trees to select the best feature for splitting.**"
      ],
      "metadata": {
        "id": "loPXYIoe3qcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "Both Gini Impurity and Entropy are metrics used in Decision Tree algorithms to measure how impure or mixed a dataset is. They help decide the best feature to split the data.\n",
        "\n",
        "**Meaning**\n",
        "\n",
        "**Gini Impurity**:\n",
        "\n",
        "-  Measures how often a randomly chosen data point would be incorrectly classified.\n",
        "\n",
        "- Lower Gini value means purer nodes.\n",
        "\n",
        "**Entropy**:\n",
        "\n",
        "- Measures the level of uncertainty or randomness in the data.\n",
        "\n",
        "- Lower entropy means less disorder.\n",
        "\n",
        "**How They Are Used**\n",
        "\n",
        "- Both are used to select the best split in a decision tree.\n",
        "\n",
        "- The split that results in maximum purity (least impurity/uncertainty) is chosen.\n",
        "\n",
        "| Aspect         | Gini Impurity                 | Entropy                       |\n",
        "| -------------- | ----------------------------- | ----------------------------- |\n",
        "| Concept        | Misclassification probability | Uncertainty or randomness     |\n",
        "| Interpretation | How impure the node is        | How unpredictable the node is |\n",
        "| Computation    | Simpler and faster            | Slightly more complex         |\n",
        "| Speed          | Faster in practice            | Slower compared to Gini       |\n",
        "| Used in        | CART algorithm                | ID3 and C4.5 algorithms       |\n",
        "| Bias           | Prefers larger partitions     | More balanced splits          |\n",
        "\n",
        "\n",
        "**Which One Is Better?**\n",
        "\n",
        "- Gini Impurity is preferred when speed and simplicity matter.\n",
        "\n",
        "- Entropy is preferred when information gain and interpretability are important.\n",
        "\n",
        "- In practice, both often produce similar trees.\n",
        "\n",
        "**Gini Impurity measures the chance of misclassification, while Entropy measures the uncertainty in the data; both are used to choose the best split in decision trees.**"
      ],
      "metadata": {
        "id": "CSpI6tUJ4vYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Pre-pruning is a technique used in decision trees to stop the tree from growing too deep during training. The idea is to prevent overfitting by setting certain rules in advance that decide when the tree should stop splitting.\n",
        "\n",
        "Instead of allowing the decision tree to grow until it perfectly classifies all training data, pre-pruning halts further splits if they do not significantly improve the model’s performance. This helps the model remain simple, faster, and more generalizable to new data.\n",
        "\n",
        "Common conditions used in pre-pruning include:\n",
        "\n",
        "- Limiting the maximum depth of the tree\n",
        "\n",
        "- Requiring a minimum number of samples in a node before splitting\n",
        "\n",
        "- Stopping splits when improvement is very small\n",
        "\n",
        "**Why Pre-Pruning Is Important**\n",
        "\n",
        "- Reduces overfitting\n",
        "\n",
        "- Improves model performance on unseen data\n",
        "\n",
        "- Makes the model faster and easier to interpret\n",
        "\n",
        "**Pre-pruning is a method that stops a decision tree from growing too large during training to avoid overfitting and improve generalization.**"
      ],
      "metadata": {
        "id": "PakKdOlq6ADp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access  feature_importances_.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IejyEU7w6Xb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create feature names\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Train Decision Tree Classifier with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qhPyJEK6qs1",
        "outputId": "b7d98aed-9034-4f2a-90fa-62502012c565"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "\n",
        "- The model uses Gini Impurity to decide the best splits.\n",
        "\n",
        "- Feature importance shows how much each feature contributes to decision-making.\n",
        "\n",
        "- Higher value = more important feature.\n",
        "\n",
        "- In this example, petal length and petal width are the most important features.\n",
        "\n",
        "**This program trains a Decision Tree using Gini Impurity and prints feature importances to identify which features contribute most to predictions.**"
      ],
      "metadata": {
        "id": "c8SWiuLI6ycz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, but it is most commonly used for classification problems.\n",
        "\n",
        "SVM works by finding a decision boundary (called a hyperplane) that best separates data points of different classes. The main goal of SVM is to choose a boundary that maximizes the margin, which is the distance between the boundary and the nearest data points from each class.\n",
        "\n",
        "The data points that lie closest to the decision boundary are called support vectors. These points are crucial because they directly influence the position of the hyperplane. If these points move, the decision boundary also changes.\n",
        "\n",
        "SVM is powerful because it can handle:\n",
        "\n",
        "- High-dimensional data\n",
        "\n",
        "- Non-linearly separable data using kernel functions\n",
        "\n",
        "- Small datasets with clear class separation\n",
        "\n",
        "Overall, SVM focuses on finding the most optimal and robust boundary, making it effective for complex classification tasks.\n",
        "\n",
        "**Support Vector Machine is a supervised learning algorithm that finds the optimal hyperplane which maximizes the margin between different classes using support vectors.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ar8_1a4z69Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data.\n",
        "\n",
        "In many real-world problems, data cannot be separated by a straight line or flat plane. Instead of trying to draw a complex boundary in the original space, the kernel trick maps the data into a higher-dimensional space where a linear separation becomes possible.\n",
        "\n",
        "The key idea is that SVM does not actually compute the transformation explicitly. Instead, it uses a kernel function to calculate the similarity between data points as if they were in a higher-dimensional space. This makes the process computationally efficient.\n",
        "\n",
        "**Why Kernel Trick is Important**\n",
        "\n",
        "- Helps SVM handle complex and non-linear patterns\n",
        "\n",
        "- Avoids expensive calculations of high-dimensional transformations\n",
        "\n",
        "- Makes SVM flexible for different types of data\n",
        "\n",
        "**Common Types of Kernels**\n",
        "\n",
        "- **Linear Kernel** – for linearly separable data\n",
        "\n",
        "- **Polynomial Kernel** – for curved decision boundaries\n",
        "\n",
        "- **RBF (Gaussian) Kernel** – for complex, non-linear patterns\n",
        "\n",
        "- **Sigmoid Kernel** – similar to neural networks\n",
        "\n",
        "\n",
        "**The kernel trick allows SVM to solve non-linear classification problems by implicitly mapping data into a higher-dimensional space where it can be linearly separated.**"
      ],
      "metadata": {
        "id": "hM7rfzbw7Ulv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n"
      ],
      "metadata": {
        "id": "HOfC0sYS7yZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Linear Kernel SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexXkFaH7-Rz",
        "outputId": "50aa9210-5f4a-4d0b-d24b-a84a353c77eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel SVM Accuracy: 0.9814814814814815\n",
            "RBF Kernel SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "- Linear SVM performs very well when data is nearly linearly separable.\n",
        "\n",
        "- RBF SVM captures non-linear patterns better and achieved higher accuracy on the Wine dataset."
      ],
      "metadata": {
        "id": "m8ngggj08Aml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "Naïve Bayes is a probabilistic machine learning classifier based on Bayes’ Theorem. It is mainly used for classification tasks such as spam detection, sentiment analysis, and text classification.\n",
        "\n",
        "It works by calculating the probability of each class for a given data point and then predicting the class with the highest probability.\n",
        "\n",
        "It is called “Naïve” because it makes a strong assumption that all features are independent of each other, given the class label.\n",
        "In real-world data, this assumption is usually not true, but surprisingly, Naïve Bayes still performs very well in many practical applications.\n",
        "\n",
        "**Why Naïve Bayes is Popular**\n",
        "\n",
        "- Simple and fast to train\n",
        "\n",
        "- Works well with large datasets\n",
        "\n",
        "- Performs especially well for text and categorical data\n",
        "\n",
        "- Requires less training data compared to many other algorithms\n",
        "\n",
        "**Key Point**:\n",
        "\n",
        "**Naïve Bayes is called “naïve” because it assumes feature independence, an assumption that simplifies computation but rarely holds in real data.**"
      ],
      "metadata": {
        "id": "nBGDBr0i8H7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "Naïve Bayes classifiers differ mainly in the type of data they are designed to handle and the probability distribution they assume for the features.\n",
        "\n",
        "**Gaussian Naïve Bayes**\n",
        "\n",
        "Gaussian Naïve Bayes is used when the features are continuous numerical values. It assumes that the data for each feature follows a normal (Gaussian) distribution.\n",
        "\n",
        "- Common use cases:\n",
        "\n",
        "\n",
        "1.   Medical data (e.g., blood pressure, temperature)\n",
        "2.   Sensor readings\n",
        "3. Any dataset with real-valued features\n",
        "\n",
        "**Multinomial Naïve Bayes**\n",
        "\n",
        "Multinomial Naïve Bayes is designed for discrete count-based data.\n",
        "It is most commonly used in text classification, where features represent word counts or term frequencies.\n",
        "\n",
        "- Common use cases:\n",
        "\n",
        "\n",
        "\n",
        "1.   Spam detection\n",
        "2.   Document classification\n",
        "3.   Bag-of-Words and TF-IDF models\n",
        "\n",
        "**Bernoulli Naïve Bayes**\n",
        "\n",
        "Bernoulli Naïve Bayes works with binary features (0 or 1).\n",
        "It checks whether a feature is present or absent, rather than how many times it appears.\n",
        "\n",
        "- Common use cases:\n",
        "\n",
        "1. Binary text features (word present or not)\n",
        "2. Yes/No type data\n",
        "3. Simple feature presence detection\n",
        "\n",
        "**Comparison**\n",
        "\n",
        "| Type           | Data Type       | Feature Representation | Typical Use Case        |\n",
        "| -------------- | --------------- | ---------------------- | ----------------------- |\n",
        "| Gaussian NB    | Continuous      | Real-valued            | Medical, numerical data |\n",
        "| Multinomial NB | Discrete counts | Frequency-based        | Text classification     |\n",
        "| Bernoulli NB   | Binary          | Presence/absence       | Binary text features    |\n",
        "\n",
        "\n",
        "**Key Point**:\n",
        "\n",
        "**Gaussian NB is used for continuous data, Multinomial NB for count-based text data, and Bernoulli NB for binary features.**"
      ],
      "metadata": {
        "id": "QNKUXd1P8XRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "KPdV1Ogr9bNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Naïve Bayes:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1apo5FGS9gwF",
        "outputId": "9b8b87d1-36e5-40e7-9fa0-b9d52eb13d4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**:\n",
        "\n",
        "- Gaussian Naïve Bayes performs well on continuous medical data\n",
        "\n",
        "- It achieves high accuracy with very fast training\n",
        "\n",
        "- Suitable for baseline medical classification models"
      ],
      "metadata": {
        "id": "Wxu-7SMg9jk4"
      }
    }
  ]
}